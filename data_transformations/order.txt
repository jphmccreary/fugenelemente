definitely should have filtered out the data before decmpounding but here's order of ops i've used:
    (done) decompound
    (done) pull out "words" w spaces
    (done) lemmatize
    (done) pull out numbers
    (done) unfuge candidates added
    (done) reshape
        this step pulls out unused data fields and stores ycs in a dict hashed by base word text
    (done) use only modifier+head pairs? len(splits) == 2
        reduced total size of dumps by ~400MB (1.8GB -> 1.44GB)
    (done) use only words with at least one coocurrence? count > 1
        reduced total size of dumps by ~1GB! (1.44GB -> 477MB)
    ?use only words that can be a noun?
    pull out named entities (when to perform this step?) - this could perhaps be nullified if we only use defuged modifiers that exist as a lemma in the vocabulary
